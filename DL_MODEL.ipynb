{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa4b2de",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b513c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from catboost import CatBoostRegressor\n",
    "import ngboost as ngb\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b320d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sedag\\AppData\\Local\\Temp\\ipykernel_28220\\1501782592.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data_train.sort_values(by='date', inplace=True)\n",
      "C:\\Users\\sedag\\AppData\\Local\\Temp\\ipykernel_28220\\1501782592.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_data_test.sort_values(by='date', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------done get_data_splits---------------\n",
      "Epoch 1/100\n",
      "1868/1868 [==============================] - 29s 9ms/step - loss: 0.2291 - mae: 0.2291 - accuracy: 0.0000e+00 - val_loss: 0.2188 - val_mae: 0.2188 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2285 - mae: 0.2285 - accuracy: 0.0000e+00 - val_loss: 0.2187 - val_mae: 0.2187 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2283 - mae: 0.2283 - accuracy: 0.0000e+00 - val_loss: 0.2183 - val_mae: 0.2183 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1868/1868 [==============================] - 16s 9ms/step - loss: 0.2282 - mae: 0.2282 - accuracy: 0.0000e+00 - val_loss: 0.2179 - val_mae: 0.2179 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2280 - mae: 0.2280 - accuracy: 0.0000e+00 - val_loss: 0.2177 - val_mae: 0.2177 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2280 - mae: 0.2280 - accuracy: 0.0000e+00 - val_loss: 0.2176 - val_mae: 0.2176 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1868/1868 [==============================] - 16s 9ms/step - loss: 0.2278 - mae: 0.2278 - accuracy: 0.0000e+00 - val_loss: 0.2174 - val_mae: 0.2174 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2277 - mae: 0.2277 - accuracy: 0.0000e+00 - val_loss: 0.2173 - val_mae: 0.2173 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "1868/1868 [==============================] - 17s 9ms/step - loss: 0.2277 - mae: 0.2277 - accuracy: 0.0000e+00 - val_loss: 0.2172 - val_mae: 0.2172 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "1868/1868 [==============================] - 18s 9ms/step - loss: 0.2275 - mae: 0.2275 - accuracy: 0.0000e+00 - val_loss: 0.2171 - val_mae: 0.2171 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1868/1868 [==============================] - 18s 9ms/step - loss: 0.2275 - mae: 0.2275 - accuracy: 0.0000e+00 - val_loss: 0.2171 - val_mae: 0.2171 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2275 - mae: 0.2275 - accuracy: 0.0000e+00 - val_loss: 0.2171 - val_mae: 0.2171 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2274 - mae: 0.2274 - accuracy: 0.0000e+00 - val_loss: 0.2173 - val_mae: 0.2173 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2274 - mae: 0.2274 - accuracy: 0.0000e+00 - val_loss: 0.2173 - val_mae: 0.2173 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2274 - mae: 0.2274 - accuracy: 0.0000e+00 - val_loss: 0.2172 - val_mae: 0.2172 - val_accuracy: 5.3551e-04 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1868/1868 [==============================] - 16s 9ms/step - loss: 0.2275 - mae: 0.2275 - accuracy: 0.0000e+00 - val_loss: 0.2187 - val_mae: 0.2187 - val_accuracy: 5.3551e-04 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2189 - val_mae: 0.2189 - val_accuracy: 5.3551e-04 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2190 - val_mae: 0.2190 - val_accuracy: 5.3551e-04 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2191 - val_mae: 0.2191 - val_accuracy: 5.3551e-04 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2191 - val_mae: 0.2191 - val_accuracy: 5.3551e-04 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2192 - val_mae: 0.2192 - val_accuracy: 5.3551e-04 - lr: 4.0000e-05\n",
      "Epoch 22/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2193 - val_mae: 0.2193 - val_accuracy: 5.3551e-04 - lr: 4.0000e-05\n",
      "Epoch 23/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2193 - val_mae: 0.2193 - val_accuracy: 5.3551e-04 - lr: 4.0000e-05\n",
      "Epoch 24/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2193 - val_mae: 0.2193 - val_accuracy: 5.3551e-04 - lr: 4.0000e-05\n",
      "Epoch 25/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2193 - val_mae: 0.2193 - val_accuracy: 5.3551e-04 - lr: 4.0000e-05\n",
      "Epoch 26/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2193 - val_mae: 0.2193 - val_accuracy: 5.3551e-04 - lr: 8.0000e-06\n",
      "Epoch 27/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2193 - val_mae: 0.2193 - val_accuracy: 5.3551e-04 - lr: 8.0000e-06\n",
      "Epoch 28/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2193 - val_mae: 0.2193 - val_accuracy: 5.3551e-04 - lr: 8.0000e-06\n",
      "Epoch 29/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 8.0000e-06\n",
      "Epoch 30/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 8.0000e-06\n",
      "Epoch 31/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.6000e-06\n",
      "Epoch 32/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.6000e-06\n",
      "Epoch 33/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.6000e-06\n",
      "Epoch 34/100\n",
      "1868/1868 [==============================] - 16s 9ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.6000e-06\n",
      "Epoch 35/100\n",
      "1868/1868 [==============================] - 17s 9ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.6000e-06\n",
      "Epoch 36/100\n",
      "1868/1868 [==============================] - 16s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 3.2000e-07\n",
      "Epoch 37/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 3.2000e-07\n",
      "Epoch 38/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 3.2000e-07\n",
      "Epoch 39/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 3.2000e-07\n",
      "Epoch 40/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 3.2000e-07\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 6.4000e-08\n",
      "Epoch 42/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 6.4000e-08\n",
      "Epoch 43/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 6.4000e-08\n",
      "Epoch 44/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 6.4000e-08\n",
      "Epoch 45/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 6.4000e-08\n",
      "Epoch 46/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.2800e-08\n",
      "Epoch 47/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.2800e-08\n",
      "Epoch 48/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.2800e-08\n",
      "Epoch 49/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.2800e-08\n",
      "Epoch 50/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.2800e-08\n",
      "Epoch 51/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 52/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 53/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 54/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 55/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 56/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 57/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 58/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 59/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 60/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 61/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 62/100\n",
      "1868/1868 [==============================] - 15s 8ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 63/100\n",
      "1868/1868 [==============================] - 17s 9ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 64/100\n",
      "1868/1868 [==============================] - 16s 9ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 65/100\n",
      "1868/1868 [==============================] - 17s 9ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 66/100\n",
      "1868/1868 [==============================] - 17s 9ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 67/100\n",
      "1868/1868 [==============================] - 16s 9ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 68/100\n",
      "1868/1868 [==============================] - 16s 9ms/step - loss: 0.2272 - mae: 0.2272 - accuracy: 0.0000e+00 - val_loss: 0.2194 - val_mae: 0.2194 - val_accuracy: 5.3551e-04 - lr: 1.0000e-08\n",
      "Epoch 68: early stopping\n",
      "------done create_and_train_model---------------\n",
      "2335/2335 [==============================] - 6s 3ms/step - loss: 0.2256 - mae: 0.2256 - accuracy: 1.0711e-04\n",
      "1107/1107 [==============================] - 3s 2ms/step - loss: 0.2142 - mae: 0.2142 - accuracy: 9.3168e-04\n",
      "train_accuracy: [0.22560587525367737, 0.22560587525367737, 0.00010710508649935946]\n",
      "test_accuracy: [0.21424196660518646, 0.21424196660518646, 0.0009316770010627806]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "def preprocess_data():\n",
    "    all_data = pd.read_csv('asos_train.csv', encoding='cp949')\n",
    "    \n",
    "#     ['영암에프원태양광b', '[서인천]발전부지내 태양광 1단계', '[서인천]발전부지내 태양광 1단계ESS',\n",
    "#        '[서인천]발전부지내 태양광 2단계', '군산복합2단계태양광', '군산복합2단계태양광ESS', '대전 학하 연료전지',\n",
    "#        '삼랑진태양광#1', '삼랑진태양광#2', '서인천연료전지', '서인천연료전지2',\n",
    "#        '세종시수질복원센터태양광연계 ESS', '수질복원센터태양광', '의왕 연료전지', '장흥풍력', '천안청수연료전지']\n",
    "\n",
    "    relevant_loc = ['영암에프원태양광b','[서인천]발전부지내 태양광 1단계','[서인천]발전부지내 태양광 2단계', \n",
    "                    '삼랑진태양광#1', '삼랑진태양광#2', '수질복원센터태양광']\n",
    "\n",
    "    all_data = all_data[all_data['name'].isin(relevant_loc)]\n",
    "\n",
    "    all_data.fillna(-1, inplace=True)\n",
    "    \n",
    "    all_data['date'] = pd.to_datetime(all_data['date'])\n",
    "    all_data['year'] = all_data['date'].dt.year\n",
    "    all_data['month'] = all_data['date'].dt.month\n",
    "    all_data['hour'] = all_data['date'].dt.hour\n",
    "\n",
    "\n",
    "    # Index(['power', 'date', 'name', 'capacities', 'asos_num', 'land', 'power/land',\n",
    "    #        'location', 'Date/Time', 'Temp(C)', 'Prec(mm)', 'Wind_speed(m/s)',\n",
    "    #        'Wind_Direction(16 compass points)', 'Humidity', 'Vapor_pressure(hPa)',\n",
    "    #        'Dew_Point(C)', 'Local Atmospheric Pressure(hPa)',\n",
    "    #        'Sea-level_Pressure(hPa)', 'sunshine(hr)', 'Solar_Radiation(MJ/m2)',\n",
    "    #        'Snowfall(cm)', '3-hour_Fresh_Snowfall', 'Cloud_Cover(1/10)',\n",
    "    #        'Mid-Low_Cloud_Cover', 'Cloud_Form', 'Lowest_Cloud_Height(100m)',\n",
    "    #        'Visibility(10m)', 'Ground_Condition', 'Phenomenon_Number',\n",
    "    #        'Ground Temp', '5cm_Underground_Temp', '10cm_Underground_Temp',\n",
    "    #        '20cm_Underground_Temp', '30cm_Underground_Temp'],\n",
    "    #       dtype='object')\n",
    "\n",
    "    drop_columns = [ \"asos_num\", \"location\", \"power\", \"capacities\", \"land\",\n",
    "                   'Wind_Direction(16 compass points)', 'Humidity', 'Vapor_pressure(hPa)', \"Date/Time\",\n",
    "                    'Local Atmospheric Pressure(hPa)',\n",
    "                   'Sea-level_Pressure(hPa)', \n",
    "                   '3-hour_Fresh_Snowfall',\n",
    "                   'Mid-Low_Cloud_Cover', 'Cloud_Form', 'Lowest_Cloud_Height(100m)',\n",
    "                   'Visibility(10m)', 'Ground_Condition', 'Phenomenon_Number',\n",
    "                   'Ground Temp', '5cm_Underground_Temp', '10cm_Underground_Temp',\n",
    "                   '20cm_Underground_Temp', '30cm_Underground_Temp']\n",
    "\n",
    "    all_data = all_data.drop(columns=drop_columns)\n",
    "    \n",
    "    all_data['power/land'] = normalize_power_with_scaler(all_data[['power/land']])\n",
    "    \n",
    "    all_data_train = all_data[all_data['year'].isin([2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020])]\n",
    "    all_data_test = all_data[all_data['year'].isin([2021,2022])]\n",
    "    \n",
    "    all_data_train.sort_values(by='date', inplace=True)\n",
    "    all_data_test.sort_values(by='date', inplace=True)\n",
    "\n",
    "\n",
    "    return all_data_train, all_data_test\n",
    "\n",
    "def normalize_power_with_scaler(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "def get_data_splits(train_scaled, test_scaled):\n",
    "    train_X = train_scaled.drop(columns=['power/land', 'name', 'date'])\n",
    "    test_X = test_scaled.drop(columns=['power/land', 'name', 'date'])\n",
    "    train_y = train_scaled['power/land']\n",
    "    test_y = test_scaled['power/land']\n",
    "    #X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "    #((39205, 16), (39205,), (4470, 16), (4470,))\n",
    "    print('------done get_data_splits---------------')\n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "\n",
    "def get_possible_combinations():\n",
    "    config = [[True, False], [16, 32, 64, 128], [8, 16, 32]]\n",
    "    print('------done get_possible combinations---------------')\n",
    "    return list(itertools.product(*config))\n",
    "\n",
    "def create_and_train_model(train_X, train_y, test_X, test_y, n_neurons, n_batch_size, dropout, additional_layer):\n",
    "    train_X2 = train_X.values.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    train_y2 = train_y.values.reshape((train_y.shape[0]))\n",
    "\n",
    "    train_X3, valid_X, train_y3, valid_y = train_test_split(train_X2, train_y2, test_size = 0.2, shuffle = False)\n",
    "\n",
    "    test_X2 = test_X.values.reshape((test_X.shape[0], 1,test_X.shape[1]))\n",
    "    test_y2 = test_y.values#.reshape((test_y.shape[0]))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=n_neurons, return_sequences=True, input_shape=(1, train_X.shape[1])))\n",
    "    model.add(Dropout(dropout))\n",
    "    if additional_layer:\n",
    "        model.add(GRU(units=n_neurons, return_sequences=True))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(GRU(units=n_neurons, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GRU(units=n_neurons, return_sequences=True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GRU(units=n_neurons, return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mae', metrics=['mae', 'accuracy'])\n",
    "\n",
    "    es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=20)\n",
    "    mc = ModelCheckpoint(f_name, monitor='val_loss', mode='min', verbose=0, save_best_only=True)\n",
    "    rl = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00000001)\n",
    "\n",
    "    model.fit(train_X3, train_y3, validation_data = (valid_X, valid_y), epochs=100, verbose=1, batch_size=n_batch_size, callbacks=[es, mc, rl], shuffle=False)\n",
    "    print('------done create_and_train_model---------------')\n",
    "    return model\n",
    "\n",
    "def evaluate_hyperparameters(train_X, train_y):\n",
    "    possible_combinations = get_possible_combinations()\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "    losses = np.empty(len(possible_combinations))\n",
    "\n",
    "    for i, (additional_layer, n_neurons, n_batch_size) in enumerate(possible_combinations):\n",
    "        print('--------------------------------------------------------------------')\n",
    "        print(f'Combination #{i+1}: {additional_layer, n_neurons, n_batch_size}\\n')\n",
    "\n",
    "        val_loss = []\n",
    "        for j, (train_index, val_index) in enumerate(kfold.split(train_X)):\n",
    "            val_train_X = train_X.iloc[train_index, :]\n",
    "            val_train_y = train_y.iloc[train_index]\n",
    "\n",
    "            model = create_and_train_model(val_train_X, val_train_y, n_neurons, n_batch_size, 0.2, additional_layer)\n",
    "\n",
    "            val_X = train_X.iloc[val_index, :].values.reshape((-1, 1, train_X.shape[1]))\n",
    "            val_y = train_y.iloc[val_index].values\n",
    "\n",
    "            val_accuracy = model.evaluate(val_X, val_y, verbose=0)\n",
    "            val_loss.append(val_accuracy[1])\n",
    "\n",
    "            print(f'{j+1}-FOLD ====> val acc: {val_accuracy[1]}')\n",
    "\n",
    "        mean_val_loss = np.mean(val_loss)\n",
    "        print(f'Mean validation loss: {mean_val_loss}')\n",
    "        losses[i] = mean_val_loss\n",
    "\n",
    "    best_index = np.argmin(losses)\n",
    "    print(f\"Best hyperparameters: {possible_combinations[best_index]} with validation RMSE: {losses[best_index]}\")\n",
    "\n",
    "    return possible_combinations[best_index], losses[best_index]\n",
    "\n",
    "# Applying the functions\n",
    "\n",
    "def main():\n",
    "    train_scaled_df, test_scaled_df = preprocess_data()\n",
    "    train_X, train_y, test_X, test_y = get_data_splits(train_scaled_df, test_scaled_df)\n",
    "    model = create_and_train_model(train_X, train_y, test_X, test_y, n_neurons, n_batch_size, dropout, additional_layer)\n",
    "    train_X2 = train_X.values.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    train_y2 = train_y.values.reshape((train_y.shape[0]))\n",
    "    test_X2 = test_X.values.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    test_y2 = test_y.values\n",
    "    #best_hyperparameters, best_loss = evaluate_hyperparameters(train_X, train_y)\n",
    "    train_accuracy = model.evaluate(train_X2, train_y2, verbose=1)\n",
    "    test_accuracy = model.evaluate(test_X2, test_y2, verbose=1)\n",
    "    return train_accuracy, test_accuracy\n",
    "\n",
    "# If this script is being run as the main module, execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    additional_layer = True\n",
    "    n_neurons = 64\n",
    "    n_batch_size = 32\n",
    "    combination_no = 9\n",
    "    dropout=0.2\n",
    "\n",
    "    f_name = f'best_model{combination_no}.hdf5'\n",
    "\n",
    "    train_accuracy, test_accuracy = main()\n",
    "    print(f\"train_accuracy: {train_accuracy}\")\n",
    "    print(f\"test_accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf9fc27a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd5 in position 29: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model9.hdf5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m all_data_train, all_data_test \u001b[38;5;241m=\u001b[39m preprocess_data()\n\u001b[0;32m     10\u001b[0m X_train \u001b[38;5;241m=\u001b[39m all_data_train\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpower/land\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\saving\\saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    205\u001b[0m         filepath,\n\u001b[0;32m    206\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    208\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:703\u001b[0m, in \u001b[0;36mis_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns whether the path is a directory or not.\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;124;03m  True, if the path is a directory; False otherwise\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 703\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mIsDirectory(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError:\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd5 in position 29: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('best_model9.hdf5')\n",
    "all_data_train, all_data_test = preprocess_data()\n",
    "\n",
    "X_train = all_data_train.drop(columns=['power/land', 'name', 'date'])\n",
    "y_train = all_data_train['power/land']\n",
    "X_test = all_data_test.drop(columns=['power/land', 'name', 'date'])\n",
    "y_test = all_data_test['power/land']\n",
    "test_X_values = X_test.values  # DataFrame을 NumPy 배열로 변환\n",
    "test_X_reshaped = test_X_values.reshape((test_X_values.shape[0], 1, test_X_values.shape[1])) \n",
    "\n",
    "\n",
    "yhat = model.predict(test_X_reshaped)\n",
    "X_test['pred'] = yhat\n",
    "\n",
    "for location in test_data['name'].unique():\n",
    "    print(f\"Testing for location {location}...\")\n",
    "\n",
    "    location_data = X_test[X_test['name']==location]\n",
    "    pred = location_data['pred']\n",
    "    actual = location_data['power/land']\n",
    "\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    print(f\"mae is {mae}\")\n",
    "\n",
    "    location_data['date'] = pd.to_datetime(location_data['date'])\n",
    "    location_data.set_index('date', inplace = True)\n",
    "\n",
    "    daily_actual_sum = location_data.resample('D').sum()['power/land']\n",
    "    daily_predicted_sum = location_data.resample('D').sum()['pred']\n",
    "\n",
    "    corr_coefficient, p_value = calculate_pearsonr(daily_actual_sum, daily_predicted_sum)\n",
    "\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.plot(daily_actual_sum, label = 'actual', color = 'red')\n",
    "    plt.plot(daily_predicted_sum, label = 'predicted', color = 'blue')\n",
    "    plt.title(f\"in {location} pred/actual\")\n",
    "    plt.ylabel('power')\n",
    "    plt.legend()\n",
    "\n",
    "    corr_text = f\"Pearson correlation coefficient: {corr_coefficient:.6f}\\nP-value: {p_value:.6f}\"\n",
    "    plt.annotate(corr_text, xy=(0.75, 0.15), xycoords='axes fraction', fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"black\", facecolor=\"aliceblue\"))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3690fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
